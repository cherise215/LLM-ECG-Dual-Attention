{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "## use a LSTM model to predict the current ecg sequence or future sequence\n",
    "class Sequence(nn.Module):\n",
    "    def __init__(self,input_feature_dim =1, num_embedding=64):\n",
    "        super(Sequence, self).__init__()\n",
    "        self.num_embedding = num_embedding\n",
    "        self.lstm1 = nn.LSTMCell(input_feature_dim, num_embedding)\n",
    "        self.lstm2 = nn.LSTMCell(num_embedding, num_embedding)\n",
    "        self.linear = nn.Linear(num_embedding, input_feature_dim)\n",
    "\n",
    "    def forward(self, input, future = 0):\n",
    "        outputs = []\n",
    "        h_t = torch.zeros(input.size(0), self.num_embedding, dtype=torch.float32)\n",
    "        c_t = torch.zeros(input.size(0), self.num_embedding, dtype=torch.float32)\n",
    "        h_t2 = torch.zeros(input.size(0), self.num_embedding, dtype=torch.float32)\n",
    "        c_t2 = torch.zeros(input.size(0), self.num_embedding, dtype=torch.float32)\n",
    "\n",
    "        for input_t in input.split(1, dim=1):\n",
    "            h_t, c_t = self.lstm1(input_t, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "        for i in range(future):# if we should predict the future\n",
    "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True, False, False,  True,  True],\n",
      "         [ True, False, False,  True,  True],\n",
      "         [ True, False, False,  True,  True],\n",
      "         [ True, False, False,  True,  True],\n",
      "         [ True, False, False,  True,  True]],\n",
      "\n",
      "        [[ True, False, False,  True,  True],\n",
      "         [ True, False, False,  True,  True],\n",
      "         [ True, False, False,  True,  True],\n",
      "         [ True, False, False,  True,  True],\n",
      "         [ True, False, False,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import  torch.nn as nn\n",
    "batch_size = 2\n",
    "num_leads = 5\n",
    "feature_dim =512\n",
    "time_length =1024\n",
    "input = torch.randn(num_leads, batch_size, feature_dim)\n",
    "mask_lead = torch.ones(batch_size, num_leads,time_length)\n",
    "mask_lead[:,[0,3,4]] = 0\n",
    "mask_lead = torch.mean(mask_lead,dim=2) ## (batch_size, num_leads)\n",
    "\n",
    "# print('mask lead\\n', mask_lead)\n",
    "lead_attn_mask = torch.zeros((batch_size,num_leads,num_leads),device =input.device)\n",
    "lead_attn_mask.masked_fill_(mask_lead.unsqueeze(1)==0, 1)\n",
    "lead_attn_mask=lead_attn_mask>0\n",
    "print(lead_attn_mask)\n",
    "## duplicate the mask to match the number of heads\n",
    "num_heads =8\n",
    "lead_attn_mask = lead_attn_mask.unsqueeze(0).repeat(num_heads, 1, 1, 1)\n",
    "lead_attn_mask = lead_attn_mask.view(-1, num_leads, num_leads)\n",
    "# print ('lead_attn_mask\\n', lead_attn_mask.shape)\n",
    "lead_mha = nn.MultiheadAttention(embed_dim=feature_dim,num_heads =num_heads)\n",
    "weighted_input, attention  = lead_mha(input,input,input,attn_mask = lead_attn_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "age_code = torch.nn.functional.one_hot(torch.tensor(7),num_classes=8)\n",
    "print(age_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## output\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "n_features = 64\n",
    "hidden_size = 12\n",
    "num_layers = 2\n",
    "rnn = nn.RNN(input_size=n_features, hidden_size=hidden_size, num_layers=num_layers,batch_first=True)\n",
    "batch_size =5\n",
    "len_seq = 1024\n",
    "\n",
    "## initial hidden state:\n",
    "initial_state = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "input = torch.randn(batch_size, 1,n_features)\n",
    "output, hn = rnn(input,initial_state)\n",
    "print (output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvRNN(nn.Module):\n",
    "    def __init__(self, input_dim, timesteps, output_dim, kernel_size1=7, kernel_size2=5, kernel_size3=3, \n",
    "                 n_channels1=32, n_channels2=32, n_channels3=32, n_units1=32, n_units2=32, n_units3=32, if_multi_lead=False,out_ch = 12):\n",
    "        super().__init__()\n",
    "        self.avg_pool1 = nn.AvgPool1d(2, 2)\n",
    "        self.avg_pool2 = nn.AvgPool1d(4, 4)\n",
    "        self.conv11 = nn.Conv1d(input_dim, n_channels1, kernel_size=kernel_size1)\n",
    "        self.conv12 = nn.Conv1d(n_channels1, n_channels1, kernel_size=kernel_size1)\n",
    "        self.conv21 = nn.Conv1d(input_dim, n_channels2, kernel_size=kernel_size2)\n",
    "        self.conv22 = nn.Conv1d(n_channels2, n_channels2, kernel_size=kernel_size2)\n",
    "        self.conv31 = nn.Conv1d(input_dim, n_channels3, kernel_size=kernel_size3)\n",
    "        self.conv32 = nn.Conv1d(n_channels3, n_channels3, kernel_size=kernel_size3)\n",
    "        self.gru1 = nn.GRU(n_channels1, n_units1, batch_first=True)\n",
    "        self.gru2 = nn.GRU(n_channels2, n_units2, batch_first=True)\n",
    "        self.gru3 = nn.GRU(n_channels3, n_units3, batch_first=True)\n",
    "        self.if_multi_lead = if_multi_lead\n",
    "        if not if_multi_lead:\n",
    "            self.linear1 = nn.Linear(n_units1+n_units2+n_units3, output_dim)\n",
    "            self.linear2 = nn.Linear(input_dim*timesteps, output_dim)\n",
    "        else:\n",
    "            self.linear1 = nn.ModuleList()\n",
    "            self.linear2 = nn.ModuleList()\n",
    "            self.out_ch = out_ch\n",
    "            for i in range(self.out_ch):\n",
    "                self.linear1.append(nn.Linear(n_units1+n_units2+n_units3, output_dim))\n",
    "                self.linear2.append(nn.Linear(input_dim*timesteps, output_dim))\n",
    "        \n",
    "        self.zp11 = nn.ConstantPad1d(((kernel_size1-1), 0), 0)\n",
    "        self.zp12 = nn.ConstantPad1d(((kernel_size1-1), 0), 0)\n",
    "        self.zp21 = nn.ConstantPad1d(((kernel_size2-1), 0), 0)\n",
    "        self.zp22 = nn.ConstantPad1d(((kernel_size2-1), 0), 0)\n",
    "        self.zp31 = nn.ConstantPad1d(((kernel_size3-1), 0), 0)\n",
    "        self.zp32 = nn.ConstantPad1d(((kernel_size3-1), 0), 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # line1\n",
    "        y1 = self.zp11(x)\n",
    "        y1 = torch.relu(self.conv11(y1))\n",
    "        y1 = self.zp12(y1)\n",
    "        y1 = torch.relu(self.conv12(y1))\n",
    "        y1 = y1.permute(0, 2, 1)\n",
    "        out, h1 = self.gru1(y1)\n",
    "        # line2\n",
    "        y2 = self.avg_pool1(x)\n",
    "        y2 = self.zp21(y2)\n",
    "        y2 = torch.relu(self.conv21(y2))\n",
    "        y2 = self.zp22(y2)\n",
    "        y2 = torch.relu(self.conv22(y2))\n",
    "        y2 = y2.permute(0, 2, 1)\n",
    "        out, h2 = self.gru2(y2)\n",
    "        # line3 \n",
    "        y3 = self.avg_pool2(x)\n",
    "        y3 = self.zp31(y3)\n",
    "        y3 = torch.relu(self.conv31(y3))\n",
    "        y3 = self.zp32(y3)\n",
    "        y3 = torch.relu(self.conv32(y3))\n",
    "        y3 = y3.permute(0, 2, 1)\n",
    "        out, h3 = self.gru3(y3)\n",
    "        h = torch.cat([h1[-1], h2[-1], h3[-1]], dim=1)\n",
    "        if self.if_multi_lead:\n",
    "            output_list = []\n",
    "            for i in range(self.out_ch):\n",
    "                out = self.linear1[i](h)\n",
    "                output_list.append(out)\n",
    "            out = torch.stack(output_list, dim=1)\n",
    "        else:\n",
    "            out = self.linear1(h)\n",
    "            # out2 = self.linear2(x.contiguous().view(x.shape[0], -1))\n",
    "            # out = out1 + out2\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 12, 1024])\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "class LinearDecoder(nn.Module):\n",
    "    def __init__(self, input_feature_dim, output_length, out_ch=1,decompose=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.out_ch = out_ch\n",
    "        self.decompose = decompose\n",
    "        self.linear1 = nn.ModuleList()\n",
    "        self.linear2 = nn.ModuleList()\n",
    "        if decompose:\n",
    "            assert input_feature_dim%2==0\n",
    "            input_feature_dim = input_feature_dim//2\n",
    "            for i in range(self.out_ch):\n",
    "                self.linear1.append(nn.Linear(input_feature_dim, output_length))\n",
    "                self.linear2.append(nn.Linear(input_feature_dim, output_length))\n",
    "        else:\n",
    "            for i in range(self.out_ch):\n",
    "                self.linear1.append(nn.Linear(input_feature_dim, output_length))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: [Batch, feature_dim]\n",
    "        return: [Batch, out_ch, output_length]\n",
    "        '''\n",
    "        output = []\n",
    "        size = x.size(1)\n",
    "        for i in range(self.out_ch):\n",
    "            if self.decompose: \n",
    "                out = self.linear1[i](x[:,:size//2])\n",
    "                output_2 = self.linear2[i](x[:,size//2:])\n",
    "                out = out + output_2\n",
    "            else:\n",
    "                out = self.linear1[i](x)\n",
    "            output.append(out)\n",
    "        output = torch.stack(output, dim=1)\n",
    "        return output\n",
    "input_data = torch.randn(5,64)\n",
    "model = LinearDecoder(input_feature_dim =64, output_length=1024,out_ch=12,decompose=False)\n",
    "output = model(input_data)\n",
    "print (output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ConvRNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[39m=\u001b[39m ConvRNN(\u001b[39m12\u001b[39m, \u001b[39m1024\u001b[39m, \u001b[39m1024\u001b[39m,if_multi_lead\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,out_ch\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m)(torch\u001b[39m.\u001b[39mrandn(\u001b[39m5\u001b[39m, \u001b[39m1024\u001b[39m,\u001b[39m12\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m (out\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ConvRNN' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "out = ConvRNN(12, 1024, 1024,if_multi_lead=True,out_ch=12)(torch.randn(5, 1024,12))\n",
    "print (out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append('../..')\n",
    "from multi_modal_heart.model.ecg_net import doubleECGNet\n",
    "\n",
    "output = doubleECGNet(decoder_type=\"linear\",decoder_outdim=12)\n",
    "output = (torch.randn(5, 12,1024))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model = Sequence(input_feature_dim=1)\n",
    "test_input = torch.randn(10,512)\n",
    "red = seq_model(test_input, future=0)\n",
    "print (red.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "import torch.nn as nn\n",
    "import torch\n",
    "n_features =512\n",
    "batch_size =10\n",
    "signal_length = 1024//32\n",
    "num_lead = 12\n",
    "input = torch.randn(batch_size, 512, 12,signal_length)\n",
    "use_attention_mask = True\n",
    "## make it as L,N,F, cross time attention\n",
    "# attn_mask = torch.ones(signal_length, signal_length)\n",
    "# attn_mask = torch.triu(attn_mask, diagonal=1)\n",
    "time_input = input.permute(3,0,2,1).reshape(signal_length,batch_size,-1)\n",
    "## \n",
    " \n",
    "time_encoder = PositionalEncoding(d_model = n_features*num_lead,dropout=0.5)\n",
    "time_input = time_encoder(time_input)\n",
    "mha = nn.MultiheadAttention(embed_dim=n_features*num_lead,num_heads = 8)\n",
    "\n",
    "if use_attention_mask:\n",
    "    attn_mask = torch.ones(signal_length,signal_length)\n",
    "    attn_mask = 1-torch.triu(attn_mask, diagonal=1)\n",
    "else:\n",
    "    attn_mask = None\n",
    "cross_time,attn_output_weights = mha(time_input,time_input,time_input,attn_mask=attn_mask)\n",
    "print (cross_time.shape)\n",
    "## leadwise attention:\n",
    "lead_input = input.permute(2,0,1,3).reshape(num_lead,batch_size,-1)\n",
    "embedding = n_features*signal_length\n",
    "\n",
    "lead_mha = nn.MultiheadAttention(embed_dim=embedding,num_heads = 8)\n",
    "\n",
    "lead_encoder = PositionalEncoding(d_model = n_features*signal_length,dropout=0.5)\n",
    "lead_input = lead_encoder(lead_input)\n",
    "cross_lead,lead_attn_output_weights = lead_mha(lead_input,lead_input,lead_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class AttentionPool1d(nn.Module):\n",
    "    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None, dropout_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim+ 1, embed_dim) / embed_dim ** 0.5)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_p  = dropout_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(2, 0, 1)  # NCL -> (L)NC\n",
    "\n",
    "        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (L)NC\n",
    "        # print (x.shape)\n",
    "        # print (self.positional_embedding[:, None, :].to(x.dtype).shape)\n",
    "        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n",
    "        x, _ = F.multi_head_attention_forward(\n",
    "            query=x[:1], key=x, value=x,\n",
    "            embed_dim_to_check=x.shape[-1],\n",
    "            num_heads=self.num_heads,\n",
    "            q_proj_weight=self.q_proj.weight,\n",
    "            k_proj_weight=self.k_proj.weight,\n",
    "            v_proj_weight=self.v_proj.weight,\n",
    "            in_proj_weight=None,\n",
    "            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n",
    "            bias_k=None,\n",
    "            bias_v=None,\n",
    "            add_zero_attn=False,\n",
    "            dropout_p=  self.dropout_p,\n",
    "            out_proj_weight=self.c_proj.weight,\n",
    "            out_proj_bias=self.c_proj.bias,\n",
    "            use_separate_proj_weight=True,\n",
    "            training=self.training,\n",
    "            need_weights=False\n",
    "        )\n",
    "        return x.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cross lead attention\n",
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.2, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no linear layer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 1024])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import torch\n",
    "sys.path.append('../..')\n",
    "from multi_modal_heart.model.ecg_net import doubleECGNet\n",
    "\n",
    "from multi_modal_heart.model.ecg_net_attention import ECGAttentionAE\n",
    "\n",
    "ecg_net = ECGAttentionAE(num_leads=12, time_steps=1024, z_dims=512, \n",
    "                         linear_out=512, downsample_factor=4, upsample_factor=4,\n",
    "                         base_feature_dim=4,if_VAE=False,use_attention_pool=False,no_linear_in_E=True)\n",
    "\n",
    "ecg_net(torch.randn(1,12,1024)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from multi_modal_heart.model.ecg_net import ECGAE\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, network,task_name,input_key_name=\"input_seq\", target_key_name=\"cleaned_seq\", future_key_name=\"next_seq\", grad_clip=False, warmup=100,\n",
    "        max_iters=2000,batch_size=128, **kwargs):\n",
    "        super().__init__()  \n",
    "        self.network = network\n",
    "        self.task_name = task_name\n",
    "\n",
    "\n",
    "ae=ECGAE(encoder_type =\"ms_resnet\",in_channels=8,ECG_length=1024,embedding_dim=256,latent_code_dim=64,add_time=False,\n",
    "               apply_method=\"\",decoder_outdim=8,time_dim=0,act = nn.GELU(),encoder_mha=True)\n",
    "\n",
    "model = LitAutoEncoder(network=ae,task_name=\"ECGAE\",input_key_name=\"input_seq\", target_key_name=\"cleaned_seq\")\n",
    "model.load_from_checkpoint(\"/home/engs2522/project/multi-modal-heart/log/dae_64+mha_ms_resnet/checkpoints/epoch=85-step=2924.ckpt\")\n",
    "\n",
    "# encoder_vec = ecg_net.encodeECG(torch.randn(4,12,1024),mask = torch.randn(4,12,1024),auto_pad_input=True)\n",
    "# print (encoder_vec.shape)\n",
    "# decoder_vec = ecg_net.decodeECG(encoder_vec)\n",
    "# print (decoder_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test resnet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from multi_modal_heart.model.ecg_net import ECG_ResNetencoder\n",
    "resencoder = ECG_ResNetencoder(in_channels=12,ECG_length=1024,embedding_dim=256,output_dim=64)\n",
    "resencoder(torch.randn(4,12,1024)).shape\n",
    "resencoder.get_features_after_pooling(torch.randn(4,12,1024)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_modal_heart.model.ecg_net_attention import ECGAttentionAE\n",
    "ecg_net = ECGAttentionAE(num_leads=12, time_steps=1024, z_dims=512, linear_out=64, downsample_factor=5, base_feature_dim=4,if_VAE=False,no_linear_in_E=True)\n",
    "ecg_net.encoder.get_features_after_pooling(torch.randn(4,12,1024)).shape\n",
    "# ecg_net.encoder(torch.randn(4,12,1024)).shape\n",
    "ecg_net(torch.randn(4,12,1024)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, Callback\n",
    "\n",
    "class OneCycleLR(Callback):\n",
    "    def __init__(self, max_lr, total_steps, pct_start=0.3, anneal_strategy='cos'):\n",
    "        super(OneCycleLR, self).__init__()\n",
    "        self.max_lr = max_lr\n",
    "        self.total_steps = total_steps\n",
    "        self.pct_start = pct_start\n",
    "        self.anneal_strategy = anneal_strategy\n",
    "\n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        optimizer = trainer.optimizers[0]\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=self.max_lr,\n",
    "            total_steps=self.total_steps,\n",
    "            pct_start=self.pct_start,\n",
    "            anneal_strategy=self.anneal_strategy\n",
    "        )\n",
    "        trainer.lr_schedulers = [scheduler]  # Set the scheduler to the trainer\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module, outputs):\n",
    "        scheduler = trainer.lr_schedulers[0]\n",
    "        scheduler.step()  # Step the learning rate scheduler after each epoch\n",
    "\n",
    "\n",
    "# Your PyTorch Lightning module\n",
    "class MyModule(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        # Initialize your model, loss, etc.\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Define your training logic\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "# Training script\n",
    "model = MyModule()\n",
    "callbacks = [\n",
    "    LearningRateMonitor(logging_interval='step'),\n",
    "    OneCycleLR(max_lr=0.1, total_steps=100, pct_start=0.3, anneal_strategy='cos')\n",
    "]\n",
    "trainer = pl.Trainer(devices=1,callbacks=callbacks, max_epochs=10)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize the latent space\n",
    "from nomic import atlas\n",
    "import numpy as np\n",
    "\n",
    "num_embeddings = 1000\n",
    "embeddings = np.random.rand(num_embeddings, 256)\n",
    "\n",
    "categories = ['rhizome', 'cartography', 'lindenstrauss']\n",
    "data = [{'category': categories[i % len(categories)], 'id': i}\n",
    "            for i in range(len(embeddings))]\n",
    "\n",
    "# project = atlas.map_embeddings(embeddings=embeddings,\n",
    "#                                 data=data,\n",
    "#                                 id_field='id',\n",
    "#                                 colorable_fields=['category']\n",
    "#                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "reducer = umap.UMAP()\n",
    "## transform embeddings to 2D\n",
    "umap_embeddings = reducer.fit_transform(embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], cmap='Spectral')\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP projection of the Autoencoder embeddings', fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_modal_heart.model.basic_conv1d import MyResidualBlock1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_modal_heart.model.ecg_net_attention import ECGAttentionAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load Y_train.csv data: 17111 samples\n",
      "load Y_validate.csv data: 2156 samples\n",
      "load Y_test.csv data: 2163 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/engs2522/project/multi-modal-heart/multi_modal_heart/model/../../multi_modal_heart/ECG/ecg_dataset.py:46: DtypeWarning: Columns (22,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.label_csv_df = pd.read_csv(label_csv_path)\n"
     ]
    }
   ],
   "source": [
    "## build classification network\n",
    "\n",
    "## start training\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from torch.utils.data import DataLoader\n",
    "from multi_modal_heart.ECG.ecg_dataset import ECGDataset\n",
    "## initialize a dataloader (all data)\n",
    "data_folder = \"/home/engs2522/project/multi-modal-heart/multi_modal_heart/data/ptbxl/\"\n",
    "train_data_statement_path = os.path.join(data_folder,\"/home/engs2522/project/multi-modal-heart/multi_modal_heart/data/ptbxl/raw_split/Y_train.csv\")\n",
    "validate_data_statement_path = os.path.join(data_folder,\"/home/engs2522/project/multi-modal-heart/multi_modal_heart/data/ptbxl/raw_split/Y_validate.csv\")\n",
    "test_data_statement_path = os.path.join(data_folder,\"/home/engs2522/project/multi-modal-heart/multi_modal_heart/data/ptbxl/raw_split/Y_test.csv\")\n",
    "\n",
    "\n",
    "data_loaders = []\n",
    "sampling_rate=500\n",
    "batch_size  = 12\n",
    "max_seq_len = 608\n",
    "data_proc_config={\n",
    "                \"if_clean\":True,\n",
    "                 }\n",
    "data_aug_config={\n",
    "                \"noise_frequency_list\":[5,20,100,150,175],\n",
    "                \"noise_amplitude_range\":[0.,0.2],\n",
    "                \"powerline_frequency_list\":[50],\n",
    "                \"powerline_amplitude_range\":[0.,0.05],\n",
    "                \"artifacts_amplitude_range\":[0.,0.1],\n",
    "                \"artifacts_frequency_list\":[5,10],\n",
    "                \"artifacts_number_range\":[0,3],\n",
    "                \"linear_drift_range\":[0.,0.1],\n",
    "                \"random_prob\":0.5,\n",
    "                \"if_mask_signal\":True, \n",
    "                \"mask_whole_lead_prob\":0.2,\n",
    "                \"lead_mask_prob\":0.2,\n",
    "                \"region_mask_prob\":0.15,\n",
    "                \"mask_length_range\":[0.08, 0.18],\n",
    "                \"mask_value\":0.0,\n",
    "                \n",
    "                }\n",
    "for label_csv_path in [train_data_statement_path,validate_data_statement_path,test_data_statement_path]:\n",
    "    if_test =\"test\" in label_csv_path.split(\"/\")[-1]\n",
    "    dataset = ECGDataset(data_folder,label_csv_path=label_csv_path,\n",
    "                         use_median_wave=True, ## set to median wave, then it has 600 samples for each lead, when sampling rate is 100\n",
    "                          sampling_rate=sampling_rate,\n",
    "                          max_seq_len=max_seq_len,\n",
    "                          augmentation= not if_test,\n",
    "                          data_proc_config=data_proc_config,\n",
    "                          data_aug_config=data_aug_config,)\n",
    "    data_loader = DataLoader(dataset=dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            num_workers=0,\n",
    "                            shuffle = not if_test,\n",
    "                            drop_last= not if_test,\n",
    "                            )\n",
    "    print ('load {} data: {} samples'.format(label_csv_path.split(\"/\")[-1],len(dataset)))\n",
    "    data_loaders.append(data_loader)\n",
    "    \n",
    "train_loader, validate_loader, test_loader = data_loaders[0],data_loaders[1],data_loaders[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/engs2522/local/conda/envs/pytorch3d/lib/python3.9/site-packages/neurokit2/signal/signal_distort.py:301: NeuroKitWarning: Skipping requested noise frequency  of 100 Hz since it cannot be resolved at  the sampling rate of 500 Hz. Please increase  sampling rate to 1000 Hz or choose  frequencies smaller than or equal to 50.0 Hz.\n",
      "  warn(\n",
      "/home/engs2522/local/conda/envs/pytorch3d/lib/python3.9/site-packages/neurokit2/signal/signal_distort.py:301: NeuroKitWarning: Skipping requested noise frequency  of 150 Hz since it cannot be resolved at  the sampling rate of 500 Hz. Please increase  sampling rate to 1500 Hz or choose  frequencies smaller than or equal to 50.0 Hz.\n",
      "  warn(\n",
      "/home/engs2522/local/conda/envs/pytorch3d/lib/python3.9/site-packages/neurokit2/signal/signal_distort.py:301: NeuroKitWarning: Skipping requested noise frequency  of 175 Hz since it cannot be resolved at  the sampling rate of 500 Hz. Please increase  sampling rate to 1750 Hz or choose  frequencies smaller than or equal to 50.0 Hz.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zong error: index 600 is out of bounds for axis 0 with size 600']\n",
      "['zong error: index 600 is out of bounds for axis 0 with size 600']\n",
      "['zong error: index 600 is out of bounds for axis 0 with size 600']\n",
      "['zong error: index 600 is out of bounds for axis 0 with size 600']\n",
      "['zong error: index 600 is out of bounds for axis 0 with size 600']\n",
      "['zong error: index 600 is out of bounds for axis 0 with size 600']\n",
      "['zong error: index 600 is out of bounds for axis 0 with size 600']\n",
      "['zong error: index 600 is out of bounds for axis 0 with size 600']\n",
      "['zong error: index 600 is out of bounds for axis 0 with size 600']\n",
      "['zong error: index 600 is out of bounds for axis 0 with size 600']\n",
      "['gamboa error: index -1 is out of bounds for axis 0 with size 0', 'zong error: index 600 is out of bounds for axis 0 with size 600']\n",
      "['zong error: index 600 is out of bounds for axis 0 with size 600']\n",
      "torch.Size([12, 12, 608])\n",
      "torch.Size([12, 12, 608])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print (data[\"input_seq\"].shape)\n",
    "    print (data[\"cleaned_seq\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "# wfdb.rdsamp(\"/home/engs2522/project/multi-modal-heart/multi_modal_heart/data/ptbxl/records100/00000/00001_lr\")\n",
    "wfdb.rdsamp(\"/home/engs2522/project/multi-modal-heart/multi_modal_heart/data/ptbxl/median_beats/unig/11000/011848_medians\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load UKB_train.csv data: 100 samples\n",
      "load UKB_val.csv data: 50 samples\n",
      "load UKB_test.csv data: 50 samples\n"
     ]
    }
   ],
   "source": [
    "## UKB dataset\n",
    "## build classification network\n",
    "\n",
    "## start training\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from torch.utils.data import DataLoader\n",
    "from multi_modal_heart.ECG.ecg_UKB_dataset import ECGUKBDataset\n",
    "\n",
    "data_loaders = []\n",
    "sampling_rate=100\n",
    "batch_size  = 12\n",
    "max_seq_len = 1024\n",
    "data_proc_config={\n",
    "                \"if_clean\":False,\n",
    "                 }\n",
    "data_aug_config={\n",
    "                \"noise_frequency_list\":[5,20,100,150,175],\n",
    "                \"noise_amplitude_range\":[0.,0.2],\n",
    "                \"powerline_frequency_list\":[50],\n",
    "                \"powerline_amplitude_range\":[0.,0.05],\n",
    "                \"artifacts_amplitude_range\":[0.,0.1],\n",
    "                \"artifacts_frequency_list\":[5,10],\n",
    "                \"artifacts_number_range\":[0,3],\n",
    "                \"linear_drift_range\":[0.,0.1],\n",
    "                \"random_prob\":0.5,\n",
    "                \"if_mask_signal\":True, \n",
    "                \"mask_whole_lead_prob\":0.2,\n",
    "                \"lead_mask_prob\":0.2,\n",
    "                \"region_mask_prob\":0.15,\n",
    "                \"mask_length_range\":[0.08, 0.18],\n",
    "                \"mask_value\":0.0,\n",
    "                \n",
    "                }\n",
    "\n",
    "## initialize a dataloader (all data)\n",
    "data_folder = \"/home/engs2522/project/multi-modal-heart/multi_modal_heart/data/UKB/\"\n",
    "train_data_statement_path = os.path.join(data_folder,\"splits/100_norm_100_HF/UKB_train.csv\")\n",
    "validate_data_statement_path = os.path.join(data_folder,\"splits/100_norm_100_HF/UKB_val.csv\")\n",
    "test_data_statement_path = os.path.join(data_folder,\"splits/100_norm_100_HF/UKB_test.csv\")\n",
    "\n",
    "for label_csv_path in [train_data_statement_path,validate_data_statement_path,test_data_statement_path]:\n",
    "    if_test =\"test\" in label_csv_path.split(\"/\")[-1]\n",
    "    dataset = ECGUKBDataset(data_folder,label_csv_path=label_csv_path,\n",
    "                          sampling_rate=sampling_rate,\n",
    "                          use_median_wave=False,\n",
    "                          max_seq_len=max_seq_len,\n",
    "                          augmentation= not if_test,\n",
    "                          data_proc_config=data_proc_config,\n",
    "                          data_aug_config=data_aug_config,)\n",
    "    data_loader = DataLoader(dataset=dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            num_workers=0,\n",
    "                            shuffle = not if_test,\n",
    "                            drop_last= not if_test,\n",
    "                            )\n",
    "    print ('load {} data: {} samples'.format(label_csv_path.split(\"/\")[-1],len(dataset)))\n",
    "    data_loaders.append(data_loader)\n",
    "    \n",
    "train_loader, validate_loader, test_loader = data_loaders[0],data_loaders[1],data_loaders[2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torch import optim, nn\n",
    "import torch\n",
    "from torchmetrics.classification import MultilabelAUROC\n",
    " \n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from multi_modal_heart.model.ecg_net import ClassifierMLP\n",
    "from multi_modal_heart.model.ecg_net_attention import ECGAttentionAE\n",
    "from multi_modal_heart.ECG.utils import evaluate_experiment\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, network):\n",
    "        super().__init__()\n",
    "        self.network=network\n",
    "        self.latent_code_dim =64\n",
    "        self.num_classes = 5  \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class LitClassifier(pl.LightningModule):\n",
    "    def __init__(self,encoder,num_classes=5,learning_rate=1e-3,freeze_encoder=False, \n",
    "                 task_name = \"ECG_Classifier\", max_iters =20000):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        if freeze_encoder:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.freeze_encoder = freeze_encoder\n",
    "        self.learning_rate = learning_rate\n",
    "        self.downsteam_net= ClassifierMLP(input_size=64,hidden_sizes=[256],output_size=num_classes)\n",
    "   \n",
    "        self.latent_code_dim =64\n",
    "        self.test_preds = []\n",
    "        self.test_ground_truth = []\n",
    "        self.task_name = task_name\n",
    "        self.max_iters = max_iters\n",
    "        self.save_hyperparameters()\n",
    "        self.class_names= ['CD', 'HYP', 'MI', 'NORM', 'STTC']\n",
    "        ## define metrics here\n",
    "        self.macro_auroc_metric = MultilabelAUROC(num_labels=num_classes, average=\"macro\", thresholds=None)\n",
    "        self.classwise_auroc_metric = MultilabelAUROC(num_labels=num_classes, average=None, thresholds=None)\n",
    "        self.test_macro_auroc_metric = MultilabelAUROC(num_labels=num_classes, average=\"macro\", thresholds=None)\n",
    "        self.test_classwise_auroc_metric = MultilabelAUROC(num_labels=num_classes, average=None, thresholds=None)\n",
    "    \n",
    "    def forward(self, x, eval=False):\n",
    "        if self.freeze_encoder or eval:\n",
    "            self.encoder.eval()\n",
    "        else:\n",
    "            self.encoder.train()\n",
    "        latent_code = self.encoder(x)\n",
    "        return self.downsteam_net(latent_code)\n",
    "    \n",
    "    def run_task(self, batch, batch_idx, prefix_name=\"\"):\n",
    "        input = batch[\"input_seq\"]\n",
    "        target = batch[\"super_class_encoding\"].float()\n",
    "        if prefix_name==\"train\" and self.global_step>200:\n",
    "            eval=False\n",
    "        else:\n",
    "            eval=True\n",
    "        outputs_before_sigmoid = self.forward(input,eval)\n",
    "        loss = torch.nn.BCEWithLogitsLoss()(outputs_before_sigmoid,target)\n",
    "        self.log(f\"{self.task_name}/{prefix_name}_loss\", loss)\n",
    "        return loss,torch.sigmoid(outputs_before_sigmoid),target\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, _,_ = self.run_task(batch, batch_idx, prefix_name=\"train\")\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss,pred,target= self.run_task(batch, batch_idx, prefix_name=\"val\")\n",
    "        self.classwise_auroc_metric(pred,target.long())\n",
    "        self.macro_auroc_metric(pred,target.long())\n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        ## gather all the predictions and ground truth\n",
    "        macro_auc = self.macro_auroc_metric.compute()\n",
    "        self.log(f\"{self.task_name}/val_macro_auc\", macro_auc)\n",
    "        # classwise_auc = self.classwise_auroc_metric.compute()\n",
    "        print (\"macro_auc\",macro_auc.item())\n",
    "        # for i in range(classwise_auc.shape[0]):\n",
    "        #     self.log(f\"{self.task_name}/val_auc_{i}\", classwise_auc[i])\n",
    "        # self.classwise_auroc_metric.reset()\n",
    "        self.macro_auroc_metric.reset()\n",
    "        \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, pred,target = self.run_task(batch, batch_idx, prefix_name=\"test\")\n",
    "        # self.test_preds.append(pred)\n",
    "        # self.test_ground_truth.append(target)\n",
    "        self.test_classwise_auroc_metric(pred,target.long())\n",
    "        self.test_macro_auroc_metric(pred,target.long())\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW([\n",
    "            {'params':self.encoder.parameters() , 'lr': 1e-4 or self.lr,},\n",
    "            {'params':self.downsteam_net.parameters(), 'lr': self.learning_rate or self.lr},\n",
    "        ],betas=(0.9, 0.95))\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=20,\n",
    "            num_training_steps=self.trainer.estimated_stepping_batches,\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        print (self.task_name)\n",
    "  \n",
    "    def on_test_epoch_end(self):\n",
    "        ## gather all the predictions and ground truth\n",
    "        # total_test_preds = torch.cat(self.test_preds,dim=0)\n",
    "        # total_test_ground_truth = torch.cat(self.test_ground_truth,dim=0)\n",
    "        test_classwise = self.test_classwise_auroc_metric.compute()\n",
    "        test_summary = self.test_macro_auroc_metric.compute()\n",
    "        self.log(f\"{self.task_name}/test_macro_auc\", test_summary)\n",
    "        for i in range(test_classwise.shape[0]):\n",
    "            self.log(f\"{self.task_name}/test_auc_{self.class_names[i]}\", test_classwise[i])\n",
    "        print (test_summary)\n",
    "        print (test_classwise)\n",
    "        print (test_summary)\n",
    "        print (test_classwise)\n",
    "        self.test_classwise_auroc_metric.reset()\n",
    "        self.test_macro_auroc_metric.reset()\n",
    "        return test_classwise,test_summary\n",
    "n_leads = 12\n",
    "input_length = 608\n",
    "\n",
    "ecg_net = ECGAttentionAE(num_leads=n_leads, time_steps=input_length, z_dims=64,downsample_factor=5, base_feature_dim=4,if_VAE=False)\n",
    "model = LitAutoEncoder(network=ecg_net)\n",
    "model.load_from_checkpoint(\"/home/engs2522/project/multi-modal-heart/log/ECG_attention_64_ms_resnet/checkpoints/epoch=299-step=10200.ckpt\")\n",
    "   \n",
    "model.network.decoder =None ## save space\n",
    "classifier = LitClassifier(model.network.encoder,num_classes=5,freeze_encoder=False, task_name = \"freeze_encoder\",\n",
    "                           learning_rate=1e-3, max_iters =20000)\n",
    "prediction =classifier(torch.randn(5,12,608)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from finetuning_scheduler import FinetuningScheduler\n",
    "from transformers import    get_linear_schedule_with_warmup\n",
    "from pytorch_lightning.tuner import Tuner\n",
    "\n",
    "finetune_max_epochs =50\n",
    "finetune_task_name = \"finetune\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "    monitor=f\"{finetune_task_name}/val_macro_auc\",\n",
    "    min_delta=0.00,\n",
    "    patience=10,\n",
    "    verbose=False,\n",
    "    mode='min'\n",
    ")  \n",
    "\n",
    "\n",
    "\n",
    "class FineTuneLearningRateFinder(pl.callbacks.LearningRateFinder):\n",
    "    def __init__(self, milestones, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.milestones = milestones\n",
    "\n",
    "    def on_fit_start(self, *args, **kwargs):\n",
    "        return\n",
    "\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        if trainer.current_epoch in self.milestones or trainer.current_epoch == 0:\n",
    "            self.lr_find(trainer, pl_module)\n",
    "\n",
    "finetuner = pl.Trainer(accelerator=\"gpu\",\n",
    "                devices=1, max_epochs=finetune_max_epochs,fast_dev_run=True,\n",
    "                callbacks=[FineTuneLearningRateFinder(milestones=(5, 10)),early_stop_callback],\n",
    "                ) \n",
    "\n",
    "finetuner.fit(model = classifier,train_dataloaders=train_loader,val_dataloaders=validate_loader)\n",
    "## evaluate the model\n",
    "finetuner.test(classifier,test_loader)\n",
    "print (pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "result = mlb.fit_transform([[\"NORM\", \"MI\", \"HYP\",\"STTC\",\"CD\"]])\n",
    "input= [[\"NORM\"]]\n",
    "mlb.transform(input)\n",
    "mlb.classes_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "m = nn.AdaptiveAvgPool2d((1,1))\n",
    "input_data = torch.randn(1, 128, 7,64)\n",
    "m(input_data).size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
