{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. load model\n",
    "2. load test data \n",
    "3. get prediction along with attention map\n",
    "4. for predictions with correctly predicted ones, get an averaged attention map for each disease class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "sys.path.append('../..')\n",
    "from multi_modal_heart.model.ecg_net_attention import ECGEncoder,ECGAttentionAE\n",
    "from multi_modal_heart.model.ecg_net import ECGAE\n",
    "import pytorch_lightning as pl\n",
    "from multi_modal_heart.model.ecg_net import BenchmarkClassifier\n",
    "\n",
    "class LitClassifier(pl.LightningModule):\n",
    "    def __init__(self,encoder,input_dim,num_classes=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        #### add classifier if use benchmark classifier\n",
    "        self.downsteam_net = BenchmarkClassifier(input_size=input_dim,hidden_size=128,output_size=num_classes)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        latent_code = self.encoder.get_features_after_pooling(x,mask)\n",
    "        return self.downsteam_net(latent_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no linear layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/engs2522/local/conda/envs/pytorch3d/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:196: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['encoder.input_conv.0.weight', 'encoder.input_conv.0.bias', 'encoder.input_conv.1.weight', 'encoder.input_conv.1.bias', 'encoder.input_conv.3.weight', 'encoder.input_conv.3.bias', 'encoder.input_conv.4.weight', 'encoder.input_conv.4.bias', 'encoder.encoder.0.conv1.weight', 'encoder.encoder.0.bn1.weight', 'encoder.encoder.0.bn1.bias', 'encoder.encoder.0.bn1.running_mean', 'encoder.encoder.0.bn1.running_var', 'encoder.encoder.0.bn1.num_batches_tracked', 'encoder.encoder.0.conv2.weight', 'encoder.encoder.0.bn2.weight', 'encoder.encoder.0.bn2.bias', 'encoder.encoder.0.bn2.running_mean', 'encoder.encoder.0.bn2.running_var', 'encoder.encoder.0.bn2.num_batches_tracked', 'encoder.encoder.0.idfunc_1.weight', 'encoder.encoder.0.idfunc_1.bias', 'encoder.encoder.1.conv1.weight', 'encoder.encoder.1.bn1.weight', 'encoder.encoder.1.bn1.bias', 'encoder.encoder.1.bn1.running_mean', 'encoder.encoder.1.bn1.running_var', 'encoder.encoder.1.bn1.num_batches_tracked', 'encoder.encoder.1.conv2.weight', 'encoder.encoder.1.bn2.weight', 'encoder.encoder.1.bn2.bias', 'encoder.encoder.1.bn2.running_mean', 'encoder.encoder.1.bn2.running_var', 'encoder.encoder.1.bn2.num_batches_tracked', 'encoder.encoder.1.idfunc_1.weight', 'encoder.encoder.1.idfunc_1.bias', 'encoder.encoder.2.conv1.weight', 'encoder.encoder.2.bn1.weight', 'encoder.encoder.2.bn1.bias', 'encoder.encoder.2.bn1.running_mean', 'encoder.encoder.2.bn1.running_var', 'encoder.encoder.2.bn1.num_batches_tracked', 'encoder.encoder.2.conv2.weight', 'encoder.encoder.2.bn2.weight', 'encoder.encoder.2.bn2.bias', 'encoder.encoder.2.bn2.running_mean', 'encoder.encoder.2.bn2.running_var', 'encoder.encoder.2.bn2.num_batches_tracked', 'encoder.encoder.2.idfunc_1.weight', 'encoder.encoder.2.idfunc_1.bias', 'encoder.encoder.3.conv1.weight', 'encoder.encoder.3.bn1.weight', 'encoder.encoder.3.bn1.bias', 'encoder.encoder.3.bn1.running_mean', 'encoder.encoder.3.bn1.running_var', 'encoder.encoder.3.bn1.num_batches_tracked', 'encoder.encoder.3.conv2.weight', 'encoder.encoder.3.bn2.weight', 'encoder.encoder.3.bn2.bias', 'encoder.encoder.3.bn2.running_mean', 'encoder.encoder.3.bn2.running_var', 'encoder.encoder.3.bn2.num_batches_tracked', 'encoder.encoder.3.idfunc_1.weight', 'encoder.encoder.3.idfunc_1.bias', 'encoder.encoder.4.conv1.weight', 'encoder.encoder.4.bn1.weight', 'encoder.encoder.4.bn1.bias', 'encoder.encoder.4.bn1.running_mean', 'encoder.encoder.4.bn1.running_var', 'encoder.encoder.4.bn1.num_batches_tracked', 'encoder.encoder.4.conv2.weight', 'encoder.encoder.4.bn2.weight', 'encoder.encoder.4.bn2.bias', 'encoder.encoder.4.bn2.running_mean', 'encoder.encoder.4.bn2.running_var', 'encoder.encoder.4.bn2.num_batches_tracked', 'encoder.encoder.4.idfunc_1.weight', 'encoder.encoder.4.idfunc_1.bias', 'encoder.attention.time_mha.0.in_proj_weight', 'encoder.attention.time_mha.0.in_proj_bias', 'encoder.attention.time_mha.0.out_proj.weight', 'encoder.attention.time_mha.0.out_proj.bias', 'encoder.attention.time_mha.1.in_proj_weight', 'encoder.attention.time_mha.1.in_proj_bias', 'encoder.attention.time_mha.1.out_proj.weight', 'encoder.attention.time_mha.1.out_proj.bias', 'encoder.attention.time_mha.2.in_proj_weight', 'encoder.attention.time_mha.2.in_proj_bias', 'encoder.attention.time_mha.2.out_proj.weight', 'encoder.attention.time_mha.2.out_proj.bias', 'encoder.attention.time_mha.3.in_proj_weight', 'encoder.attention.time_mha.3.in_proj_bias', 'encoder.attention.time_mha.3.out_proj.weight', 'encoder.attention.time_mha.3.out_proj.bias', 'encoder.attention.time_mha.4.in_proj_weight', 'encoder.attention.time_mha.4.in_proj_bias', 'encoder.attention.time_mha.4.out_proj.weight', 'encoder.attention.time_mha.4.out_proj.bias', 'encoder.attention.time_mha.5.in_proj_weight', 'encoder.attention.time_mha.5.in_proj_bias', 'encoder.attention.time_mha.5.out_proj.weight', 'encoder.attention.time_mha.5.out_proj.bias', 'encoder.attention.time_mha.6.in_proj_weight', 'encoder.attention.time_mha.6.in_proj_bias', 'encoder.attention.time_mha.6.out_proj.weight', 'encoder.attention.time_mha.6.out_proj.bias', 'encoder.attention.time_mha.7.in_proj_weight', 'encoder.attention.time_mha.7.in_proj_bias', 'encoder.attention.time_mha.7.out_proj.weight', 'encoder.attention.time_mha.7.out_proj.bias', 'encoder.attention.time_mha.8.in_proj_weight', 'encoder.attention.time_mha.8.in_proj_bias', 'encoder.attention.time_mha.8.out_proj.weight', 'encoder.attention.time_mha.8.out_proj.bias', 'encoder.attention.time_mha.9.in_proj_weight', 'encoder.attention.time_mha.9.in_proj_bias', 'encoder.attention.time_mha.9.out_proj.weight', 'encoder.attention.time_mha.9.out_proj.bias', 'encoder.attention.time_mha.10.in_proj_weight', 'encoder.attention.time_mha.10.in_proj_bias', 'encoder.attention.time_mha.10.out_proj.weight', 'encoder.attention.time_mha.10.out_proj.bias', 'encoder.attention.time_mha.11.in_proj_weight', 'encoder.attention.time_mha.11.in_proj_bias', 'encoder.attention.time_mha.11.out_proj.weight', 'encoder.attention.time_mha.11.out_proj.bias', 'encoder.attention.time_norm_1.0.weight', 'encoder.attention.time_norm_1.0.bias', 'encoder.attention.time_norm_1.1.weight', 'encoder.attention.time_norm_1.1.bias', 'encoder.attention.time_norm_1.2.weight', 'encoder.attention.time_norm_1.2.bias', 'encoder.attention.time_norm_1.3.weight', 'encoder.attention.time_norm_1.3.bias', 'encoder.attention.time_norm_1.4.weight', 'encoder.attention.time_norm_1.4.bias', 'encoder.attention.time_norm_1.5.weight', 'encoder.attention.time_norm_1.5.bias', 'encoder.attention.time_norm_1.6.weight', 'encoder.attention.time_norm_1.6.bias', 'encoder.attention.time_norm_1.7.weight', 'encoder.attention.time_norm_1.7.bias', 'encoder.attention.time_norm_1.8.weight', 'encoder.attention.time_norm_1.8.bias', 'encoder.attention.time_norm_1.9.weight', 'encoder.attention.time_norm_1.9.bias', 'encoder.attention.time_norm_1.10.weight', 'encoder.attention.time_norm_1.10.bias', 'encoder.attention.time_norm_1.11.weight', 'encoder.attention.time_norm_1.11.bias', 'encoder.attention.time_mlp.0.fc1.weight', 'encoder.attention.time_mlp.0.fc1.bias', 'encoder.attention.time_mlp.0.fc2.weight', 'encoder.attention.time_mlp.0.fc2.bias', 'encoder.attention.time_mlp.1.fc1.weight', 'encoder.attention.time_mlp.1.fc1.bias', 'encoder.attention.time_mlp.1.fc2.weight', 'encoder.attention.time_mlp.1.fc2.bias', 'encoder.attention.time_mlp.2.fc1.weight', 'encoder.attention.time_mlp.2.fc1.bias', 'encoder.attention.time_mlp.2.fc2.weight', 'encoder.attention.time_mlp.2.fc2.bias', 'encoder.attention.time_mlp.3.fc1.weight', 'encoder.attention.time_mlp.3.fc1.bias', 'encoder.attention.time_mlp.3.fc2.weight', 'encoder.attention.time_mlp.3.fc2.bias', 'encoder.attention.time_mlp.4.fc1.weight', 'encoder.attention.time_mlp.4.fc1.bias', 'encoder.attention.time_mlp.4.fc2.weight', 'encoder.attention.time_mlp.4.fc2.bias', 'encoder.attention.time_mlp.5.fc1.weight', 'encoder.attention.time_mlp.5.fc1.bias', 'encoder.attention.time_mlp.5.fc2.weight', 'encoder.attention.time_mlp.5.fc2.bias', 'encoder.attention.time_mlp.6.fc1.weight', 'encoder.attention.time_mlp.6.fc1.bias', 'encoder.attention.time_mlp.6.fc2.weight', 'encoder.attention.time_mlp.6.fc2.bias', 'encoder.attention.time_mlp.7.fc1.weight', 'encoder.attention.time_mlp.7.fc1.bias', 'encoder.attention.time_mlp.7.fc2.weight', 'encoder.attention.time_mlp.7.fc2.bias', 'encoder.attention.time_mlp.8.fc1.weight', 'encoder.attention.time_mlp.8.fc1.bias', 'encoder.attention.time_mlp.8.fc2.weight', 'encoder.attention.time_mlp.8.fc2.bias', 'encoder.attention.time_mlp.9.fc1.weight', 'encoder.attention.time_mlp.9.fc1.bias', 'encoder.attention.time_mlp.9.fc2.weight', 'encoder.attention.time_mlp.9.fc2.bias', 'encoder.attention.time_mlp.10.fc1.weight', 'encoder.attention.time_mlp.10.fc1.bias', 'encoder.attention.time_mlp.10.fc2.weight', 'encoder.attention.time_mlp.10.fc2.bias', 'encoder.attention.time_mlp.11.fc1.weight', 'encoder.attention.time_mlp.11.fc1.bias', 'encoder.attention.time_mlp.11.fc2.weight', 'encoder.attention.time_mlp.11.fc2.bias', 'encoder.attention.mlp_norm_1.0.weight', 'encoder.attention.mlp_norm_1.0.bias', 'encoder.attention.mlp_norm_1.1.weight', 'encoder.attention.mlp_norm_1.1.bias', 'encoder.attention.mlp_norm_1.2.weight', 'encoder.attention.mlp_norm_1.2.bias', 'encoder.attention.mlp_norm_1.3.weight', 'encoder.attention.mlp_norm_1.3.bias', 'encoder.attention.mlp_norm_1.4.weight', 'encoder.attention.mlp_norm_1.4.bias', 'encoder.attention.mlp_norm_1.5.weight', 'encoder.attention.mlp_norm_1.5.bias', 'encoder.attention.mlp_norm_1.6.weight', 'encoder.attention.mlp_norm_1.6.bias', 'encoder.attention.mlp_norm_1.7.weight', 'encoder.attention.mlp_norm_1.7.bias', 'encoder.attention.mlp_norm_1.8.weight', 'encoder.attention.mlp_norm_1.8.bias', 'encoder.attention.mlp_norm_1.9.weight', 'encoder.attention.mlp_norm_1.9.bias', 'encoder.attention.mlp_norm_1.10.weight', 'encoder.attention.mlp_norm_1.10.bias', 'encoder.attention.mlp_norm_1.11.weight', 'encoder.attention.mlp_norm_1.11.bias', 'encoder.attention.lead_mha.in_proj_weight', 'encoder.attention.lead_mha.in_proj_bias', 'encoder.attention.lead_mha.out_proj.weight', 'encoder.attention.lead_mha.out_proj.bias', 'encoder.attention.lead_mlp.fc1.weight', 'encoder.attention.lead_mlp.fc1.bias', 'encoder.attention.lead_mlp.fc2.weight', 'encoder.attention.lead_mlp.fc2.bias', 'encoder.attention.lead_norm_1.weight', 'encoder.attention.lead_norm_1.bias', 'encoder.attention.norm2.weight', 'encoder.attention.norm2.bias', 'encoder.attention.attention_pool_1.1.weight', 'encoder.attention.attention_pool_1.1.bias', 'encoder.attention.attention_pool_2.1.weight', 'encoder.attention.attention_pool_2.1.bias', 'downsteam_net.model.0.weight', 'downsteam_net.model.0.bias', 'downsteam_net.model.0.running_mean', 'downsteam_net.model.0.running_var', 'downsteam_net.model.0.num_batches_tracked', 'downsteam_net.model.2.weight', 'downsteam_net.model.2.bias', 'downsteam_net.model.4.weight', 'downsteam_net.model.4.bias', 'downsteam_net.model.4.running_mean', 'downsteam_net.model.4.running_var', 'downsteam_net.model.4.num_batches_tracked', 'downsteam_net.model.6.weight', 'downsteam_net.model.6.bias'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "checkpoint_path = \"../../log_finetune/ECG_attention_512_raw_no_attention_pool_no_linear_ms_resnet/checkpoints/last-v8.ckpt\"\n",
    "# checkpoint_path = \"../../log_finetune/ECG_attention_512_raw_no_attention_pool_no_linear_add_mask_lead_attention_ms_resnet/checkpoints/last-v1.ckpt\"\n",
    "ecg_net = ECGAttentionAE(num_leads=12, time_steps=1024, z_dims=512, linear_out=512, downsample_factor=5, \n",
    "                         base_feature_dim=4,if_VAE=False,use_attention_pool=False,no_linear_in_E=True,apply_lead_mask=True)\n",
    "classification_net = LitClassifier(encoder=ecg_net.encoder,input_dim=512,num_classes=5)\n",
    "print (torch.load(checkpoint_path)[\"state_dict\"].keys())\n",
    "mm_checkpoint = torch.load(checkpoint_path)[\"state_dict\"]\n",
    "encoder_params = {(\".\").join(key.split(\".\")[1:]):value for key, value in mm_checkpoint.items() if str(key).startswith(\"encoder\")}\n",
    "classification_params = {(\".\").join(key.split(\".\")[1:]):value for key, value in mm_checkpoint.items() if str(key).startswith(\"downsteam_net\")}\n",
    "classification_net.encoder.load_state_dict(encoder_params)\n",
    "classification_net.downsteam_net.load_state_dict(classification_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "## start training\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from torch.utils.data import DataLoader\n",
    "from multi_modal_heart.ECG.ecg_dataset import ECGDataset\n",
    "## initialize a dataloader (all data)\n",
    "data_folder = \"/home/engs2522/project/multi-modal-heart/multi_modal_heart/data/ptbxl/\"\n",
    "test_data_statement_path = os.path.join(data_folder,\"/home/engs2522/project/multi-modal-heart/multi_modal_heart/data/ptbxl/raw_split/Y_test.csv\")\n",
    "\n",
    "\n",
    "data_loaders = []\n",
    "sampling_rate=100\n",
    "batch_size  = 1\n",
    "max_seq_len = 1024\n",
    "data_proc_config={\n",
    "                \"if_clean\":False,\n",
    "                 }\n",
    "data_aug_config={\n",
    "                \"noise_frequency_list\":[5,20,100,150,175],\n",
    "                \"noise_amplitude_range\":[0.,0.2],\n",
    "                \"powerline_frequency_list\":[50],\n",
    "                \"powerline_amplitude_range\":[0.,0.05],\n",
    "                \"artifacts_amplitude_range\":[0.,0.1],\n",
    "                \"artifacts_frequency_list\":[5,10],\n",
    "                \"artifacts_number_range\":[0,3],\n",
    "                \"linear_drift_range\":[0.,0.1],\n",
    "                \"random_prob\":0.5,\n",
    "                \"if_mask_signal\":True, \n",
    "                \"mask_whole_lead_prob\":0.2,\n",
    "                \"lead_mask_prob\":0.2,\n",
    "                \"region_mask_prob\":0.15,\n",
    "                \"mask_length_range\":[0.08, 0.18],\n",
    "                \"mask_value\":0.0,\n",
    "                \n",
    "}\n",
    "dataset = ECGDataset(data_folder,label_csv_path=test_data_statement_path,\n",
    "                         use_median_wave=False, ## set to median wave, then it has 600 samples for each lead, when sampling rate is 100\n",
    "                          sampling_rate=sampling_rate,\n",
    "                          max_seq_len=max_seq_len,\n",
    "                          augmentation= False,\n",
    "                          data_proc_config=data_proc_config,\n",
    "                          data_aug_config=data_aug_config,)\n",
    "data_loader = DataLoader(dataset=dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            num_workers=0,\n",
    "                            shuffle = True,\n",
    "                            drop_last= False,\n",
    "                            )\n",
    "print ('load {} data: {} samples'.format(test_data_statement_path.split(\"/\")[-1],len(dataset)))\n",
    "    \n",
    "dataset.super_class_label_converter.inverse_transform(np.array([[0,0,0,1,1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(probs,super_classes_labels, topk=1):\n",
    "    probs, label_indices = torch.topk(probs, topk)\n",
    "    probs = probs.tolist()\n",
    "    label_indices = label_indices.tolist()\n",
    "    for prob, idx in zip(probs, label_indices):\n",
    "        label = super_classes_labels[idx]\n",
    "        print(f'{label} ({idx}):', round(prob, 4))\n",
    "def calc_hamming_score(y_true, y_pred):\n",
    "    return (\n",
    "        (y_true & y_pred).sum(axis=1) / (y_true | y_pred).sum(axis=1)\n",
    "    ).mean()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_hamming_score(np.array([[0,0,1,1,0]]),np.array([[0,0,1,1,0]]))\n",
    "dataset.super_classes_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import MultilabelAccuracy\n",
    "from torchmetrics.classification import MultilabelAUROC\n",
    "from tqdm.notebook import tqdm\n",
    "metric = MultilabelAccuracy(num_labels=5)\n",
    "auroc = MultilabelAUROC(num_labels=5)\n",
    "classification_net.eval()\n",
    "label_attention_dict = {}\n",
    "for i, data in enumerate(tqdm(data_loader)):\n",
    "    ## batch size ==1\n",
    "    torch.cuda.empty_cache()\n",
    "    # ecg_id= data['ecg_id']\n",
    "    input_data=data[\"input_seq\"]\n",
    "    print (input_data.shape)\n",
    "    mask =  data[\"mask\"]\n",
    "    \n",
    "    # ## remove lead 3- data\n",
    "    # mask_lead = [0,1,4,5,6,7,8,9,10,11]\n",
    "    # mask_lead = [5]\n",
    "    input_data[:,mask_lead] = torch.zeros_like(input_data[:,mask_lead])\n",
    "    mask[:,mask_lead] = torch.zeros_like(mask[:,mask_lead])\n",
    "    \n",
    "    pred = classification_net(input_data,mask)\n",
    "    report   =data[\"report\"]\n",
    "    pred = torch.sigmoid(pred)\n",
    "    ground_truth = data[\"super_class_encoding\"]\n",
    "    # print (ground_truth.shape)\n",
    "    acc = metric.update(pred, ground_truth.long())\n",
    "    # print('pred:',torch.ones_like(ground_truth)*(pred>0.5))\n",
    "    # print('ground_truth:',ground_truth)\n",
    "    groundtruth_keys = dataset.super_class_label_converter.inverse_transform(ground_truth[[0]].cpu().numpy()) \n",
    "    print ('groundtruth',groundtruth_keys)\n",
    "    prediction_keys = dataset.super_class_label_converter.inverse_transform(torch.ones_like(ground_truth)*(pred>0.5)[[0]].cpu().numpy())\n",
    "    print (\"prediction_keys\",prediction_keys)\n",
    "    # hamming_score = calc_hamming_score(torch.ones_like(ground_truth)*(pred>0.5),ground_truth)\n",
    "    # print('hamming_score',hamming_score)\n",
    "    auroc_score = auroc.update(pred, ground_truth.long())\n",
    "    try:\n",
    "        lead_attention,_ = classification_net.encoder.get_attention()\n",
    "    except:\n",
    "        print (\"error in getting attention\")\n",
    "        break\n",
    "    # print(lead_attention)\n",
    "    print_result(pred[0], dataset.super_classes_labels,topk=5)\n",
    "    # normalized_lead_attention = (lead_attention[0]-lead_attention[0].min())/(lead_attention[0].max()-lead_attention[0].min())\n",
    "    # normalized_lead_attention = (lead_attention[0]-lead_attention[0].min())/(lead_attention[0].max()-lead_attention[0].min())\n",
    "    normalized_lead_attention = lead_attention[0]\n",
    "    for key in groundtruth_keys[0]:\n",
    "        print (key)\n",
    "        if key in prediction_keys[0]:\n",
    "            ## accurately predicted:\n",
    "            if key in label_attention_dict.keys():\n",
    "                current_map = label_attention_dict[key][0]\n",
    "                current_count = label_attention_dict[key][1]\n",
    "                if current_count>=50:continue\n",
    "                current_map += normalized_lead_attention\n",
    "                current_count += 1\n",
    "                label_attention_dict[key] = (current_map,current_count)\n",
    "            else:\n",
    "                label_attention_dict[key] = (normalized_lead_attention,1)\n",
    "\n",
    "        # print (report[5:10])\n",
    "    # print (pred[5:10])\n",
    "    # print (ground_truth[5:10])\n",
    "    # print (acc)\n",
    "    # print (auroc_score)\n",
    "    if i==0:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "## visualize the attention map for the population level model\n",
    "fig, axes = plt.subplots(5,figsize=(5,25))\n",
    "for i, (disease, attention_map_list) in enumerate(label_attention_dict.items()):\n",
    "    average_attention = attention_map_list[0]/(1.0*attention_map_list[1])\n",
    "    print (f'{disease}:  {str(attention_map_list[1])}')\n",
    "    x_ticks = [\"I\",\"II\",\"III\",\"aVR\",\"aVL\",\"aVF\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\"]\n",
    "    g = sns.heatmap(average_attention.cpu().detach().numpy(),cmap=\"YlGnBu\", ax =axes[i])\n",
    "    axes[i].set_title(disease)\n",
    "    axes[i].set_ylabel(\"leads\")\n",
    "    axes[i].set_xlabel(\"leads\")\n",
    "    ## change xticks in each axis\n",
    "    axes[i].xaxis.tick_top()\n",
    "    g.set_xticklabels(x_ticks)\n",
    "    g.set_yticklabels(x_ticks)\n",
    "\n",
    "# ## change xticks\n",
    "# plt.xticks(np.arange(0,12),x_ticks,rotation=45)\n",
    "# plt.yticks(np.arange(0,12),x_ticks,rotation=45)\n",
    "## save figure\n",
    "fig.savefig(\"./population_level_attention_map.png\",dpi=500,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
