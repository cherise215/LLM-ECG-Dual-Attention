{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.classification import MultilabelAccuracy\n",
    "from torchmetrics.classification import MultilabelAUROC\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "sys.path.append('../..')\n",
    "from multi_modal_heart.model.ecg_net_attention import ECGEncoder,ECGAttentionAE\n",
    "from multi_modal_heart.model.ecg_net import ECGAE\n",
    "from multi_modal_heart.model.ecg_net import BenchmarkClassifier\n",
    "from multi_modal_heart.ECG.ecg_dataset import ECGDataset\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "class LitClassifier(pl.LightningModule):\n",
    "    def __init__(self,encoder,input_dim,num_classes=2,lr=1e-3,freeze_encoder=False):\n",
    "        super().__init__()\n",
    "        self.lr =lr\n",
    "        self.freeze_encoder = freeze_encoder\n",
    "        self.num_classes = num_classes\n",
    "        self.encoder = encoder\n",
    "        if self.freeze_encoder:\n",
    "            self.encoder.eval()\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.accu_metric = Accuracy(task=\"multiclass\",num_classes=num_classes)\n",
    "        self.test_feature_list =[]\n",
    "        self.test_eid_list = []\n",
    "        self.preds_list = []\n",
    "        self.latent_feature_list = []\n",
    "        self.y_list = []\n",
    "        self.test_feature  = None\n",
    "        self.attention_map_classwise = {}\n",
    "        for i in range(num_classes):\n",
    "            self.attention_map_classwise[i] = []\n",
    "        #### add classifier if use benchmark classifier\n",
    "  \n",
    "        #### add classifier if use benchmark classifier\n",
    "        self.downsteam_net = BenchmarkClassifier(input_size=input_dim,hidden_size=128,output_size=num_classes)\n",
    "    def forward(self, x, mask=None):\n",
    "        latent_code = self.encoder(x,mask)\n",
    "        # print (latent_code.shape)\n",
    "        self.latent_code  = latent_code\n",
    "        return self.downsteam_net(latent_code)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[0]\n",
    "        y= batch[1]\n",
    "       \n",
    "        latent_code = self.encoder(x,mask=None)\n",
    "        self.latent_code  = latent_code\n",
    "        y_hat = self.downsteam_net(latent_code)\n",
    "        # print (y_hat.shape)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"train_loss\", loss,prog_bar=True)\n",
    "        return loss\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch[0]\n",
    "        y= batch[1]\n",
    "        eid_list = batch[2]\n",
    "        logits = self(x,mask=None)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        ## save the feature\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        last_hidden_feature = self.downsteam_net.get_features(self.latent_code)\n",
    "        eid_list = batch[2]\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        ## save the attention map\n",
    "        try:\n",
    "            lead_attention,_ = self.encoder.get_attention()\n",
    "            # print(lead_attention[i].shape)\n",
    "            ## make the [np.array] to numpy array\n",
    "            # time_attention_map = np.array(time_attention_map)\n",
    "            for i, eid in enumerate(eid_list):\n",
    "                if preds[i]==y[i]:\n",
    "                    print(f\"correct prediction: {y[i]}, eid: {eid}\")\n",
    "                    self.attention_map_classwise[y[i]].append([eid,lead_attention[i]])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"no attention map, skip\")\n",
    "            \n",
    "        self.latent_feature_list.append(self.latent_code)\n",
    "        self.test_feature_list.append(last_hidden_feature)\n",
    "        self.test_eid_list.append(eid_list)\n",
    "        self.preds_list.append(preds)\n",
    "        self.y_list.append(y)\n",
    "\n",
    "        self.accu_metric.update(preds, y)\n",
    "        return loss\n",
    "    def on_test_epoch_end(self):\n",
    "        self.log(\"test_acc\", self.accu_metric.compute())\n",
    "        ## save the feature\n",
    "        self.test_latent_feature = torch.cat(self.latent_feature_list,dim=0)\n",
    "        self.test_feature = torch.cat(self.test_feature_list,dim=0)\n",
    "        self.test_eid = torch.cat(self.test_eid_list,dim=0)\n",
    "        self.preds = torch.cat(self.preds_list,dim=0)\n",
    "        self.y = torch.cat(self.y_list,dim=0)\n",
    "        self.attention_name_map_list = []\n",
    "        ## if has attention map, save the attention map\n",
    "        for key in self.attention_map_classwise.keys():\n",
    "            group_attention = self.attention_map_classwise[key]\n",
    "            if group_attention==[]:\n",
    "                continue\n",
    "            extracted_attention_maps= [v[1] for v in group_attention]\n",
    "            if len(extracted_attention_maps)>0:\n",
    "                extracted_attention_maps = np.stack(extracted_attention_maps,axis=0)\n",
    "                average_attention_map_norm = np.mean(extracted_attention_maps,axis=0)\n",
    "                std_attention_map_norm = np.std(extracted_attention_maps,axis=0)\n",
    "                avg_name = f\"AVG_attention_map_class_{key}\"\n",
    "                std_name = f\"STD_attention_map_class_{key}\"\n",
    "\n",
    "                self.attention_name_map_list.append([avg_name,average_attention_map_norm])\n",
    "                self.attention_name_map_list.append([std_name,std_attention_map_norm])\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch[0]\n",
    "        y= batch[1]\n",
    "        y_hat = self(x,mask=None)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "    def clear_cache(self):\n",
    "        self.accu_metric.reset()\n",
    "        self.latent_feature_list = []\n",
    "        self.test_feature_list =[]\n",
    "        self.test_eid_list = []\n",
    "        self.preds_list = []\n",
    "        self.y_list = []\n",
    "        self.test_feature  = None\n",
    "        self.attention_map_classwise={}\n",
    "        for i in range(self.num_classes):\n",
    "            self.attention_map_classwise[i] = []\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(filter(lambda p: p.requires_grad, self.parameters()), lr=self.lr, weight_decay=1e-4)\n",
    "    \n",
    "def print_result(probs,super_classes_labels, topk=1):\n",
    "    probs, label_indices = torch.topk(probs, topk)\n",
    "    probs = probs.tolist()\n",
    "    label_indices = label_indices.tolist()\n",
    "    for prob, idx in zip(probs, label_indices):\n",
    "        label = super_classes_labels[idx]\n",
    "        print(f'{label} ({idx}):', round(prob, 4))\n",
    "def calc_hamming_score(y_true, y_pred):\n",
    "    return (\n",
    "        (y_true & y_pred).sum(axis=1) / (y_true | y_pred).sum(axis=1)\n",
    "    ).mean()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load MI data from UKB\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "time_steps =608\n",
    "mi_data_path = \"/home/engs2522/project/multi-modal-heart/multi_modal_heart/toolkits/ukb/non_imaging_information/MI/batched_ecg_median_wave.npy\"\n",
    "healthy_data_path = \"/home/engs2522/project/multi-modal-heart/multi_modal_heart/toolkits/ukb/non_imaging_information/non_CVD/batched_ecg_median_wave_1045.npy\"\n",
    "eid_list_mi= np.load(\"/home/engs2522/project/multi-modal-heart/multi_modal_heart/toolkits/ukb/non_imaging_information/MI/median_eid_list.npy\")\n",
    "eid_list_healthy = np.load(\"/home/engs2522/project/multi-modal-heart/multi_modal_heart/toolkits/ukb/non_imaging_information/non_CVD/selected_1045_healthy_subjects_eid_list.npy\")\n",
    "print (\"load MI data\")\n",
    "hf_data = np.load(mi_data_path)\n",
    "\n",
    "healthy_data = np.load(healthy_data_path)\n",
    "\n",
    "## check duplicate eid\n",
    "duplicate_eid = []\n",
    "\n",
    "hf_data = zscore(hf_data,axis=-1)\n",
    "healthy_data = zscore(healthy_data,axis=-1)\n",
    "hf_data = np.nan_to_num(hf_data)\n",
    "healthy_data = np.nan_to_num(healthy_data)\n",
    "\n",
    "\n",
    "## pad the data to 608\n",
    "pad_num = (time_steps-healthy_data.shape[-1])//2\n",
    "hf_data = np.pad(hf_data,((0,0),(0,0),(pad_num,pad_num)),\"constant\",constant_values=0)\n",
    "healthy_data = np.pad(healthy_data,((0,0),(0,0),(pad_num,pad_num)),\"constant\",constant_values=0)\n",
    "\n",
    "\n",
    "labels = np.concatenate([np.ones(hf_data.shape[0]),np.zeros(healthy_data.shape[0])])\n",
    "eid_full_list = np.concatenate([eid_list_mi,eid_list_healthy])\n",
    "data = np.concatenate([hf_data,healthy_data],axis=0)\n",
    "print (data.shape)\n",
    "print (labels.shape)\n",
    "print (len(eid_full_list))\n",
    "## append the eid to the label\n",
    "labels_eid = np.zeros((labels.shape[0],2))\n",
    "labels_eid[:,0] = labels\n",
    "labels_eid[:,1] = eid_full_list\n",
    "print (labels_eid.shape)\n",
    "# labels_eid.shape\n",
    "\n",
    "## split the data into train validate and test, 40% for train, 10% for validate, 50% for test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_trainval, X_test, y_trainval_eid, y_test_eid = train_test_split(data, labels_eid, test_size=0.5, random_state=42)\n",
    "X_train, X_val, y_train_eid, y_val_eid= train_test_split(X_trainval, y_trainval_eid, test_size=0.1, random_state=42)\n",
    "y_train = y_train_eid[:,0]\n",
    "y_val = y_val_eid[:,0]\n",
    "y_test = y_test_eid[:,0]\n",
    "print ('num of training data:{}, MI ratio:{}'.format(X_train.shape[0],y_train.sum()))\n",
    "print ('num of validation data:{}, MI ratio:{}'.format(X_val.shape[0],y_val.sum()))\n",
    "print ('num of test data:{}, MI ratio:{}'.format(X_test.shape[0],y_test.sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "batch_size  = 128\n",
    "tensor_x = torch.Tensor(X_train) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(y_train).long()\n",
    "tensor_eid = torch.Tensor(y_train_eid[:,1]).long()\n",
    "my_train_dataset = TensorDataset(tensor_x,tensor_y,tensor_eid) # create your datset\n",
    "my_dataloader = DataLoader(my_train_dataset,batch_size=batch_size) # create your dataloader\n",
    "\n",
    "## validation data\n",
    "tensor_x = torch.Tensor(X_val) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(y_val).long()\n",
    "tensor_eid = torch.Tensor(y_val_eid[:,1]).long()\n",
    "\n",
    "my_val_dataset =TensorDataset(tensor_x,tensor_y,tensor_eid) # create your datset\n",
    "my_val_dataloader = DataLoader(my_val_dataset,batch_size=batch_size) # create your dataloader\n",
    "\n",
    "## test data\n",
    "tensor_x = torch.Tensor(X_test) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(y_test).long()\n",
    "tensor_eid = torch.Tensor(y_test_eid[:,1]).long()\n",
    "my_test_dataset = TensorDataset(tensor_x,tensor_y,tensor_eid) # create your datset\n",
    "my_test_dataloader = DataLoader(my_test_dataset,batch_size=batch_size) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from multi_modal_heart.model.ecg_net import ECGAE\n",
    "import torch.nn as nn\n",
    "use_median_wave = True\n",
    "time_steps = 608\n",
    "model_name = \"ECG_attention_pretrained_on_recon_ECG2Text\"\n",
    "\n",
    "if model_name==\"resnet1d101_512_pretrained_recon\" or model_name==\"resnet1d101_512\":\n",
    "    ecg_net= ECGAE(encoder_type=\"resnet1d101\",in_channels=12,ECG_length=time_steps,decoder_type=\"ms_resnet\",\n",
    "                    embedding_dim=256,latent_code_dim=512,\n",
    "                    add_time=False,\n",
    "                    encoder_mha = False,\n",
    "                    apply_method=\"\",\n",
    "                    decoder_outdim=12)\n",
    "    checkpoint_path = \"../../log_median/resnet1d101_512+benchmark_classifier_ms_resnet/checkpoints/checkpoint_best_loss-v1.ckpt\"\n",
    "elif model_name==\"resnet1d101_512_pretrained_recon+ECG2Text\":\n",
    "    ecg_net= ECGAE(encoder_type=\"resnet1d101\",in_channels=12,ECG_length=time_steps,decoder_type=\"ms_resnet\",\n",
    "                    embedding_dim=256,latent_code_dim=512,\n",
    "                    add_time=False,\n",
    "                    encoder_mha = False,\n",
    "                    apply_method=\"\",\n",
    "                    decoder_outdim=12)\n",
    "    checkpoint_path = \"../../log_median/resnet1d101_512+benchmark_classifier_ms_resnet_ECG2Text/checkpoints/checkpoint_best_loss-v2.ckpt\"\n",
    "    # checkpoint_path = \"../../log_median/resnet1d101_512+benchmark_classifier_ms_resnet_ECG2Text/checkpoints/last-v2.ckpt\"\n",
    "elif model_name==\"resnet1d101_512_pretrained_classification\":\n",
    "    ecg_net= ECGAE(encoder_type=\"resnet1d101\",in_channels=12,ECG_length=time_steps,decoder_type=\"ms_resnet\",\n",
    "                    embedding_dim=256,latent_code_dim=512,\n",
    "                    add_time=False,\n",
    "                    encoder_mha = False,\n",
    "                    apply_method=\"\",\n",
    "                    decoder_outdim=12)\n",
    "    # checkpoint_path = \"/home/egs2522/project/multi-modal-heart/log_median_finetune/resnet1d101_512+benchmark_classifier_ms_resnet/checkpoints/checkpoint_best_loss.ckpt\"\n",
    "    checkpoint_path  =\"/home/engs2522/project/multi-modal-heart/log_median_finetune/resnet1d101_512+benchmark_classifier_raw_ms_resnet/checkpoints/checkpoint_best_val_macro_auc.ckpt\"\n",
    "\n",
    "elif model_name==\"resnet1d101_512_pretrained_classification+ECG2Text\":\n",
    "    ecg_net= ECGAE(encoder_type=\"resnet1d101\",in_channels=12,ECG_length=time_steps,decoder_type=\"ms_resnet\",\n",
    "                    embedding_dim=256,latent_code_dim=512,\n",
    "                    add_time=False,\n",
    "                    encoder_mha = False,\n",
    "                    apply_method=\"\",\n",
    "                    decoder_outdim=12)\n",
    "    # checkpoint_path = \"/home/engs2522/project/multi-modal-heart/log_median_finetune/resnet1d101_512+benchmark_classifier_ms_resnet_ECG2Text/checkpoints/checkpoint_best_loss.ckpt\"\n",
    "    checkpoint_path  =\"/home/engs2522/project/multi-modal-heart/log_median_finetune/resnet1d101_512+benchmark_classifier_raw_ms_resnet_ECG2Text/checkpoints/checkpoint_best_val_macro_auc-v1.ckpt\"\n",
    "\n",
    "elif model_name==\"ECG_attention_pretrained_on_classification\":\n",
    "    ecg_net  = ECGAttentionAE(num_leads=12, time_steps=time_steps, z_dims=512, linear_out=512, downsample_factor=5, base_feature_dim=4,if_VAE=False,use_attention_pool=False,\n",
    "                         no_linear_in_E=True, apply_lead_mask=False)\n",
    "    checkpoint_path = \"../../log_median_finetune/ECG_attention_512_raw_no_attention_pool_no_linear_ms_resnet/checkpoints/checkpoint_best_val_macro_auc.ckpt\"\n",
    "elif model_name==\"ECG_attention_pretrained_on_classification+ECG2Text\":\n",
    "    ecg_net  = ECGAttentionAE(num_leads=12, time_steps=time_steps, z_dims=512, linear_out=512, downsample_factor=5, base_feature_dim=4,if_VAE=False,use_attention_pool=False,\n",
    "                         no_linear_in_E=True, apply_lead_mask=False)\n",
    "    checkpoint_path = \"../../log_median_finetune/ECG_attention_512_raw_no_attention_pool_no_linear_ms_resnet_ECG2Text/checkpoints/checkpoint_best_val_macro_auc-v1.ckpt\"\n",
    "\n",
    "elif model_name==\"ECG_attention_pretrained_on_recon\" or model_name ==\"ECG_attention\":\n",
    "    ecg_net  = ECGAttentionAE(num_leads=12, time_steps=time_steps, z_dims=512, linear_out=512, \n",
    "                            downsample_factor=5, base_feature_dim=4,if_VAE=False,\n",
    "                            use_attention_pool=False,\n",
    "                         no_linear_in_E=True, apply_lead_mask=False, no_lead_attention=False,no_time_attention=False)\n",
    "    checkpoint_path = \"../../log_median/ECG_attention_512_finetuned_no_attention_pool_no_linear_ms_resnet/checkpoints/checkpoint_best_loss-v2.ckpt\"\n",
    "elif model_name==\"ECG_attention_pretrained_on_recon_ECG2Text\":\n",
    "    ecg_net  = ECGAttentionAE(num_leads=12, time_steps=time_steps, z_dims=512, linear_out=512, \n",
    "                            downsample_factor=5, base_feature_dim=4,if_VAE=False,\n",
    "                            use_attention_pool=False,\n",
    "                         no_linear_in_E=True, apply_lead_mask=False, no_lead_attention=False,no_time_attention=False)\n",
    "    print (\"load ECG attention model\")\n",
    "    checkpoint_path = \"../../log_median/ECG_attention_512_finetuned_no_attention_pool_no_linear_ms_resnet_ECG2Text/checkpoints/checkpoint_best_loss-v2.ckpt\"\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "num_classes = 2 ## for binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the autoencoder model as well\n",
    "checkpoint = torch.load(checkpoint_path)[\"state_dict\"]\n",
    "encoder_params = {(\".\").join(key.split(\".\")[2:]):value for key, value in checkpoint.items() if str(key).startswith(\"network.encoder\")}\n",
    "decoder_params = {(\".\").join(key.split(\".\")[2:]):value for key, value in checkpoint.items() if str(key).startswith(\"network.decoder\")}\n",
    "ecg_net.decoder.load_state_dict(decoder_params)\n",
    "ecg_net.encoder.load_state_dict(encoder_params)\n",
    "classification_net = LitClassifier(encoder=ecg_net.encoder,input_dim=512,num_classes=num_classes,lr=1e-3,freeze_encoder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import os\n",
    "pl.seed_everything(42)\n",
    "\n",
    "tb_logger = TensorBoardLogger( f\"./finetune_on_MI\", name=model_name, version=\"\")  \n",
    "checkpoint_dir  = os.path.join(tb_logger.log_dir,\"checkpoints\")\n",
    "\n",
    "checkpoint_callback_best_loss_min = pl.callbacks.ModelCheckpoint(dirpath=checkpoint_dir, \n",
    "                                                    filename='checkpoint_best_loss',\n",
    "                                                    save_top_k=1, monitor=\"val_loss\"\n",
    "                                                    , mode='min',save_last=True)\n",
    "\n",
    "\n",
    "callbacks=[\n",
    "    # FineTuneLearningRateFinder(milestones=[5, 10],min_lr=1e-5, max_lr=1e-3, \n",
    "    #                             mode='linear', early_stop_threshold=4.0),\n",
    "    checkpoint_callback_best_loss_min\n",
    "    ]\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\",\n",
    "                    devices=1, max_epochs=50,\n",
    "                    # logger=tb_logger,log_every_n_steps=1,check_val_every_n_epoch = 1,\n",
    "                    # callbacks=callbacks,\n",
    "                    )\n",
    "trainer.fit(classification_net, train_dataloaders=my_dataloader,val_dataloaders=my_val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## test the model\n",
    "checkpoint_path = \"/home/engs2522/project/multi-modal-heart/multi_modal_heart/tasks/finetune_on_MI/ECG_attention_pretrained_on_recon_ECG2Text/checkpoints/last-v7.ckpt\"\n",
    "classification_net.load_from_checkpoint(checkpoint_path,encoder=ecg_net.encoder,input_dim=512,num_classes=num_classes,lr=1e-3,freeze_encoder=True)\n",
    "# ## load the classifier\n",
    "# classifier_state_dict = torch.load(checkpoint_path)[\"state_dict\"]\n",
    "# classifier_state_dict = {(\".\").join(key.split(\".\")[1:]):value for key, value in classifier_state_dict.items() if str(key).startswith(\"downsteam_net\")}\n",
    "# classification_net.downsteam_net.load_state_dict(classifier_state_dict)\n",
    "classification_net.clear_cache()\n",
    "classification_net.eval()\n",
    "classification_net.freeze()\n",
    "\n",
    "\n",
    "result = trainer.test(classification_net,my_test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_modal_heart.ECG.ecg_utils import arraytodataframe,plot_multiframe_in_one_figure\n",
    "\n",
    "def plot_overlapped_multi_lead_signals(sample_y_nd,sample_x_hat_nd, labels=[\"GT \",\"Pred \"]):\n",
    "    '''\n",
    "    input: sample_y_nd: numpy array, shape: (12, time_steps)\n",
    "         sample_x_hat_nd: numpy array, shape: (12, time_steps)\n",
    "    '''\n",
    "        \n",
    "    y_df = arraytodataframe(sample_y_nd)\n",
    "    y_recon = arraytodataframe(sample_x_hat_nd)\n",
    "\n",
    "    if sample_x_hat_nd.shape[0]==12:\n",
    "        lead_names= ['I','II','III','aVR','aVL','aVF','V1','V2','V3','V4','V5','V6']\n",
    "        figure_arrangement=(4,3)\n",
    "    elif sample_x_hat_nd.shape[0]==8:\n",
    "        lead_names= ['I','II','V1','V2','V3','V4','V5','V6']\n",
    "        figure_arrangement=(4,2)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    y_df.columns = [labels[0]+k for k in lead_names]\n",
    "    y_recon.columns = [labels[1]+k for k in lead_names]\n",
    "\n",
    "    figure = plot_multiframe_in_one_figure([y_df,y_recon],figsize=(15,4), figure_arrangement=figure_arrangement, logger=None)\n",
    "    return figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_net.cuda()\n",
    "reconstructed_signal = ecg_net(x.cuda(),mask=None)\n",
    "figure = plot_overlapped_multi_lead_signals(x[0].cpu().numpy(),\n",
    "                                            reconstructed_signal[0].detach().cpu().numpy(),\n",
    "                                            labels = [\"input \",\"recon \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = next(iter(my_test_dataloader))\n",
    "classification_net.eval()\n",
    "classification_net.freeze()\n",
    "ecg_net.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the projected last hidde feature z\n",
    "classification_net.clear_cache()\n",
    "classification_net.eval()\n",
    "x = test_data[0]\n",
    "print(x.size())\n",
    "y = test_data[1]\n",
    "ecg_id = test_data[2]\n",
    "indice = 0\n",
    "classification_net.cuda()\n",
    "z = classification_net.encoder(x.cuda(),mask=None)\n",
    "y = test_data[1].cuda()\n",
    "reconstructed_signal = ecg_net.decoder(z)\n",
    "pred = classification_net.downsteam_net(z)\n",
    "pred_label = torch.argmax(pred,dim=1)\n",
    "## select those predictions with correct prediction\n",
    "print((pred_label ==y)&(y==1))\n",
    "\n",
    "z = z[(pred_label ==y)&(y==1)]\n",
    "x = x.cuda()\n",
    "filtered_x = x[(pred_label ==y)&(y==1)]\n",
    "print (z.shape)\n",
    "reconstructed_signal = reconstructed_signal[(pred_label ==y)&(y==1)]\n",
    "\n",
    "acc = classification_net.accu_metric(pred_label,y)\n",
    "print (acc)\n",
    "## convert the median wave to 16-bit float precision to save space\n",
    "# figure = plot_overlapped_multi_lead_signals(x[indice].cpu().numpy(),reconstructed_signal[indice].detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_feature = classification_net.downsteam_net.get_features(z)\n",
    "print(last_hidden_feature.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_z_dim = 28 ## the principle dimension of the hidden feature\n",
    "## adjust the latent feature z so that the hidden feature in x74 is close to 1 ##\n",
    "previous_z_74_value = last_hidden_feature[:,hidden_z_dim]\n",
    "## plot the value of z_74\n",
    "## get the minumum value and indices of z_74 \n",
    "min_z_74, min_indice = torch.min(previous_z_74_value,dim=0)\n",
    "max_z_74, max_indice = torch.max(previous_z_74_value,dim=0)\n",
    "## median value of z_74\n",
    "median_z_74, median_indice = torch.median(previous_z_74_value,dim=0)\n",
    "## get z_74 with value close to median z_74\n",
    "# median_z_74 = z[torch.abs(previous_z_74_value-median_z_74)<1e-10]\n",
    "# print(median_z_74.shape)\n",
    "print(\"the range of {} is: [{},{}]\".format(str(hidden_z_dim),min_z_74,max_z_74))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the ECG recon signal with lowest z_74 value\n",
    "figure = plot_overlapped_multi_lead_signals(filtered_x[min_indice].detach().cpu().numpy(),\n",
    "                                   filtered_x[max_indice].detach().cpu().numpy(),\n",
    "                                   labels = [\"low risk \",\"high risk \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the ECG recon signal with lowest z_74 value\n",
    "figure = plot_overlapped_multi_lead_signals(reconstructed_signal[min_indice].detach().cpu().numpy(),\n",
    "                                   reconstructed_signal[max_indice].detach().cpu().numpy(),\n",
    "                                   labels = [\"low risk \",\"high risk \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plot_overlapped_multi_lead_signals(x[median_indice].cpu().numpy(),\n",
    "                                            reconstructed_signal[max_indice].detach().cpu().numpy(),\n",
    "                                            labels = [\"medium risk \",\"high risk \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute the incidence cases's z_74 value\n",
    "import pandas as pd\n",
    "MI_HF_coxreg_df= pd.read_csv('/home/engs2522/project/multi-modal-heart/multi_modal_heart/toolkits/ukb/non_imaging_information/MI/MI_HF_coxreg_df.csv')\n",
    "MI_HF_coxreg_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incident_case = MI_HF_coxreg_df[MI_HF_coxreg_df['HF_status']==1]\n",
    "h_74_list = []\n",
    "z_feature_list =[]\n",
    "hidden_feature_list= []\n",
    "hidden_z_dim = 28\n",
    "## latent z feature \n",
    "ecg_wave_tensor_list = []\n",
    "duration_list = []\n",
    "for eid, ecg_wave in  zip(eid_list_mi, mi_data):\n",
    "    if eid in incident_case.eid.values:\n",
    "        print (eid)\n",
    "        # print (ecg_wave.shape)\n",
    "        ecg_wave_tensor = torch.Tensor(ecg_wave).unsqueeze(0).float().cuda()\n",
    "        duration = MI_HF_coxreg_df[MI_HF_coxreg_df['eid']==eid][\"time_to_HF\"].values[0]\n",
    "        ecg_wave_tensor_list.append(ecg_wave_tensor)\n",
    "        # z = classification_net.encoder.get_features_after_pooling(ecg_wave_tensor,mask=None)\n",
    "        # print(z.shape)\n",
    "        duration_list.append(duration)\n",
    "batched_ecg_wave = torch.cat(ecg_wave_tensor_list,dim=0)\n",
    "z_feature = classification_net.encoder.get_features_after_pooling(batched_ecg_wave,mask=None)\n",
    "h_feature = classification_net.downsteam_net.get_features(z_feature)\n",
    "h_74_list = h_feature[:,hidden_z_dim].detach().cpu().numpy()\n",
    "final_list  = [[h,d] for h,d in zip(h_74_list,duration_list)]\n",
    "\n",
    "z_value_time_to_hf_df = pd.DataFrame(final_list,columns=[f\"h_{hidden_z_dim}\",\"time_to_hf\"])\n",
    "# final_list[0].shape## plot the z_74 value with time to HF\n",
    "z_value_time_to_hf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the z_74 value with time to HF\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "## plot the z_74 value with time to HF as color\n",
    "ax = sns.scatterplot(x=\"time_to_hf\", y=f\"h_{hidden_z_dim}\", data=z_value_time_to_hf_df,hue=\"time_to_hf\",palette=\"cool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_selected = last_hidden_feature[[min_indice],:]\n",
    "print(h_selected.shape)\n",
    "target_h_feature = h_selected\n",
    "target_h_feature[:,hidden_z_dim] = max_z_74 ## set the value of z_74 to 1 to increase the likelihood of HF\n",
    "print (target_h_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## perform optimization of latent code z so that hidden feature h matches the target hidden feature\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import MSELoss, L1Loss\n",
    "from tqdm import tqdm\n",
    "z_selected= z[[min_indice],:].detach().clone()\n",
    "z_selected_original = z_selected.detach().clone()\n",
    "reconstructed_signal = ecg_net.decoder(z_selected)\n",
    "target_h_feature = target_h_feature.detach().clone()\n",
    "target_h_feature.requires_grad = False\n",
    "z_selected.requires_grad = True\n",
    "optimizer = AdamW([z_selected],lr=1e-3,weight_decay=1e-1)\n",
    "loss_fn = MSELoss()\n",
    "classification_net.unfreeze()\n",
    "classification_net.eval()\n",
    "for i in tqdm(range(2000)):\n",
    "    optimizer.zero_grad()\n",
    "    last_hidden_feature = classification_net.downsteam_net.get_features(z_selected)\n",
    "    loss = loss_fn(input = last_hidden_feature[:,hidden_z_dim],target = target_h_feature[:,hidden_z_dim])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%100==0:\n",
    "        print (\"loss:{}\".format(loss.item()))\n",
    "        print (\"h_28:{}\".format(last_hidden_feature[:,hidden_z_dim]))\n",
    "        ## measure the difference of the optimized value and the one before optimization\n",
    "        print (\"z difference:{}\".format(torch.sum(torch.abs(z_selected_original-z_selected))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_selected = ecg_net.decoder(z_selected)\n",
    "recon_original = ecg_net.decoder(z_selected_original)\n",
    "figure = plot_overlapped_multi_lead_signals(recon_original[min_indice].detach().cpu().numpy(),\n",
    "                                            recon_selected[min_indice].detach().cpu().numpy(),\n",
    "                                            labels = [\"before (z^0)\",\"after (z*) \"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
