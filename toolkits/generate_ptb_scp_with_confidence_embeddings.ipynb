{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents a step by step way to get text embeddings for each patient's report based on SCP codes and confidence scores provided in the PTB-XL dataset:\n",
    "\n",
    "- Input: \n",
    "    - ptbxl_database.csv\n",
    "    - scp_statements.csv\n",
    "    - Model from emilyalsentzer/Bio_ClinicalBERT\n",
    "\n",
    "- Output: \n",
    "    - A dictionary with patient ID and text embedding (dimension is 768)\n",
    "    - Saved to patient_embedding_dict_summed_SCP_structured_w_confidence.pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle \n",
    "import torch\n",
    "import pandas as pd\n",
    "import ast\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\").to(device)\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptbxl_path = \"../data/ptbxl/ptbxl_database.csv\"\n",
    "scp_statements_path = \"../data/ptbxl/scp_statements.csv\"\n",
    "assert os.path.exists(ptbxl_path) and os.path.exists(scp_statements_path), \"ptbxl_database.csv and scp_statements.csv must be downloaded from https://physionet.org/content/ptb-xl/1.0.1/ and placed in the data/ptbxl folder\"\n",
    "\n",
    "report_df = pd.read_csv(ptbxl_path)\n",
    "report_df.head(5)\n",
    "\n",
    "def convert_string_to_dict(string):\n",
    "    return ast.literal_eval(string)\n",
    "report_df['scp_codes_dict'] = report_df['scp_codes'].apply(convert_string_to_dict)\n",
    "print (\"len of patient\",len(report_df))\n",
    "report_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterate over the rows to remove the scp codes with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in report_df.iterrows():\n",
    "    remove_keys = []\n",
    "    for key in row['scp_codes_dict'].keys():\n",
    "        if row['scp_codes_dict'][key]==0:\n",
    "            remove_keys.append(key)\n",
    "    for key in remove_keys:\n",
    "        row['scp_codes_dict'].pop(key)\n",
    "\n",
    "report_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(scp_statements_path)\n",
    "print(len(df))\n",
    "print(\"SCP code description\")\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for each record, construct its text embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "text_features = []\n",
    "patient_embedding_dict = {}\n",
    "## give an ordered dictionary to cache the SCP embeddings\n",
    "cache_dict = OrderedDict()\n",
    "\n",
    "for index, row in report_df.iterrows():\n",
    "    # for each patient, get the SCP codes and their confidence scores\n",
    "    sentence_count =0\n",
    "    encoded_inputs_embedding_list = []\n",
    "    uncertainty_list = []\n",
    "    total_prompt = ''\n",
    "    for key, uncertainty in row['scp_codes_dict'].items():\n",
    "        sentence_count+=1\n",
    "        scaled_uncertainty = uncertainty/100.0\n",
    "        uncertainty_list.append(scaled_uncertainty)\n",
    "        ## promp is defined here as the SCP code categorty and the SCP code description\n",
    "        string_text = \"[CLS] \"+df[df['key']==key]['Statement Category'].values[0]+':'\n",
    "        string_text+=df[df['key']==key]['SCP-ECG Statement Description'].values[0]+\" [SCP]\"\n",
    "        print(string_text)\n",
    "      \n",
    "        if key in cache_dict.keys():\n",
    "            ## if related SCP embedding is already calculated, directly use it\n",
    "            embeddings = cache_dict[key]\n",
    "            encoded_inputs_embedding_list.append(embeddings)\n",
    "        else:\n",
    "            encoded_inputs = tokenizer(string_text, add_special_tokens=False, truncation = True,return_tensors=\"pt\",max_length=100, padding = 'max_length')\n",
    "            input_ids = encoded_inputs['input_ids']\n",
    "            segments_tensors = encoded_inputs['token_type_ids']\n",
    "            attention_mask = encoded_inputs['attention_mask']\n",
    "            with torch.inference_mode():\n",
    "                model_output = model(input_ids = input_ids.to(device),attention_mask= attention_mask.to(device))\n",
    "                embeddings = model_output.last_hidden_state\n",
    "                embeddings = torch.mean(embeddings,keepdim=True,dim=1)\n",
    "                embeddings = embeddings.squeeze(0)\n",
    "            cache_dict.update({key:embeddings})\n",
    "            encoded_inputs_embedding_list.append(embeddings)\n",
    "    multi_embedding =None\n",
    "    sum_uncertainty = sum(uncertainty_list)\n",
    "    i= 0\n",
    "    ## weighted average of SCP embeddings based on confidence scores\n",
    "    for embedding,uncertainty in zip(encoded_inputs_embedding_list,uncertainty_list):\n",
    "        if i==0:\n",
    "            multi_embedding = embedding*(uncertainty/sum_uncertainty)\n",
    "        else:multi_embedding+=embedding*(uncertainty/sum_uncertainty)\n",
    "        i+=1\n",
    "    print(multi_embedding.shape) \n",
    "    ## save the dictionary with patient ecg id as key,  and its text embedding as value\n",
    "    patient_embedding_dict.update({row['ecg_id']:multi_embedding.cpu().numpy().squeeze()})\n",
    "\n",
    "with open('patient_embedding_dict_summed_SCP_structured_w_confidence.pkl', 'wb') as f:\n",
    "    pickle.dump(patient_embedding_dict, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize different text embeddings with different scp codes with UMAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from umap import UMAP\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "sns.set(style='white', context='poster', rc={'figure.figsize':(14,10)})\n",
    "\n",
    "\n",
    "## flatten the dictionary to get the embeddings\n",
    "input_batch_embeddings = torch.stack([torch.tensor(embedding).detach() for embedding in cache_dict.values()])\n",
    "feature_df  = pd.DataFrame()\n",
    "scp_codes = [key for key in cache_dict.keys()]\n",
    "print((scp_codes))\n",
    "feature_df = pd.DataFrame(scp_codes)\n",
    "# feature_df['scp_codes'] = scp_codes\n",
    "diagnostic_class = [str(df[df['key']==key]['diagnostic_class'].values[0]) for key in cache_dict.keys()]\n",
    "feature_df['diagnostic_class'] = diagnostic_class\n",
    "print((diagnostic_class))\n",
    "print(len(diagnostic_class))\n",
    "scp_statements = [df[df['key']==key]['SCP-ECG Statement Description'].values[0] for key in cache_dict.keys()]\n",
    "statement_descriptions = [df[df['key']==key]['SCP-ECG Statement Description'].values[0] for key in cache_dict.keys()]\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# proj_2d = umap_2d.fit_transform(input_batch_embeddings.detach().cpu().numpy().squeeze())\n",
    "\n",
    "sns.set(style='white', context='poster', rc={'figure.figsize':(14,10)})\n",
    "umap_2d = UMAP(random_state=42)\n",
    "fig_2d = plt.figure(figsize=(12, 12))\n",
    "proj_2d = umap_2d.fit_transform(input_batch_embeddings[:,0,:].detach().cpu().numpy())\n",
    "# df['feature'] = proj_2d.tolist()\n",
    "feature_df['feature'] = proj_2d.tolist()\n",
    "fig_2d = px.scatter(proj_2d,x=0,y=1,color = feature_df['diagnostic_class'], \n",
    "                    symbol = scp_codes, height=600,width=600)  # O)\n",
    "fig_2d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
